---
toc: true
layout: post
categories: [markdown, database, dbms, mysql, datatype]
title: Pandas Cookbook 2ed
description: 매트 해리슨, 시어도어 페트로우 지음, 크라스랩 옮김
---

# 1장 판다스 기초
## 판다스 임포트
``` python
import pandas as pd
import numpy as np
```
데이터 프레임에서 단일 열을 선택하면 결과는 시리즈(1차원 데이터셋)으로 반환된다.   
많은 시리즈 메서드가 반환하는 출력은 시리즈다. 

## 판다스 데이터프레임 해부하기
데이터프레임은 인덱스(index), 열(columns), 데이터(data)라는 세 가지 구성요소가 있다.
``` python
pd.set_option('max_columns', 40, 'max_rows', 10)
movies = pd.read_csv('../data/movie.csv')
movies.head()
```
열과 인덱스를 통칭해 축이라고 한다. 인덱스는 축 0번이고 열은 축 1번이다.
판다스는 NaN(Not a Number)을 사용해 결측치를 나타낸다.

## 데이터프레임 속성
일반적으로 데이터를 넘파이 배열로 가져올 수 있지만 모든 열이 수치가 아닌 이상 데이터프레임에 그대로 둔다.  
데이터프레임은 열들의 데이터 형식이 서로 다른 경우를 관리하는 데 이상적이지만 넘파이 배열은 그렇지 않다.

```python
columns = movies.columns
index = movies.index
data = movies.values
```
인덱스의 타입은 pandas.core.indexes.range.RangeIndex  
컬럼의 타입은 pandas.core.indexes.base.Index   
데이터의 타입은 numpy.ndarray  
인덱스와 열은 둘 다 Index의 서브클래스이다.  
판다스에는 여러 형식의 인덱스 객체가 있다. 인덱스를 지정하지 않으면 판다스는 RangeIndex를 사용한다.   
전체 값은 필요할 때까지 메모리에 로드되지 않으므로 메모리가 절약된다.   

```python
index.values
columns.values
```
판다스의 대부분은 ndarray에 크게 의존한다. 인덱스, 열, 데이터의 이면에는 넘파이 ndarray가 있다. 

## 데이터 형식 이해
데이터는 연속형과 범주형으로 크게 분류할 수 있다. 
- float : 넘파이 부동소수점수 형식으로 결측치를 지원한다.
- int : 넘파이 정수 형식으로 결측치를 지원하지 않는다.
- 'Int64' : Null 값을 지원하는 판다스 정수 형식이다.
- object : 문자열(과 혼합형식)을 저장하는 넘파이 형식이다.
- 'category' : 판다스 범주형으로, 결측치를 지원한다.
- bool : 넘파이 불리언 형식으로 결측치를 지원하지 않는다.(None은 False, np.nan은 True로 취급)
- 'boolean' : Null 값을 지원하는 판다스 불리언 형식이다.
- datetime64[ns] : 넘파이 날짜 형식으로 결측치(NaT)를 지원한다.

```python
movies.dtypes # Series
movies.dtypes.value_counts() # Series
moives.info() # None Type
```

데이터프레임의 각 열별로 하나의 데이터 형식이 나열된다.  
판다스는 데이터를 표현하기 위한 적정 메모리 용량과 상관없이 핵심 수치 형식인 정수와 부동소수점수를 나타내는 데 64비트를 사용한다.  
열 전체가 정수 0으로만 구성돼 있더라도 여전히 데이터 형식인 int64인 것이다.  
.value_counts 메서드가 .dtypes 속성에서 호출되면 데이터프레임의 모든 데이터 형식의 개수를 반환한다.  
object 데이터 형식으로 된 열은 유효한 모든 파이썬 객체를 포함할 수 있다.  
info 메서드는 null이 아닌 값의 개수와 함께 데이터 형식 정보를 출력한다. 또한 데이터프레임에서 사용된 메모리 크기가 나열된다.  
카테고리 형식은 가능한 가지수로만 이뤄진 고정 개수의 문자열을 처리하고자 만들어졌다. 

## 열 선택
데이터 프레임에서 단일 열을 선택하면 시리즈가 반환된다. 이 시리즈는 1차원 데이터로 인덱스와 데이터로만 구성된다. 

```python
movies['director_name'] # Series
movies.director_name # Series
movies.loc[:, 'director_name'] # Series
movies.iloc[:, 1] # Series
movies['director_name'].index
movies['director_name'].dtype
movies['director_name'].size
movies['director_name'].name
movies['director_name'].apply(type).unique()
```

시리즈를 추출하는 데 다양한 방법이 있다.  .loc와 .iloc 속성을 사용할 수도 있다.   
전자는 열 이름을 끄집어내는 데 사용할 수 있고, 후자는 위치로 지정한다.  
판다스 문서상에서 이 둘은 각각 '라벨 기반'과 '위치 기반'으로 부른다.  
.loc의 사용법은 콤마(,)를 사용해 행과 열 모두를 지정하는 것이다.   
.iloc도 행과 열 선택자를 모두 선택한다.   
적절한 속성을 사용하면 시리즈의 인덱스, 형식, 길이, 이름도 볼 수 있다.  
.unique 메서드를 체인시켜 director_name 열 중에서 고유한 형식만 살펴볼 수 있다.

판다스 데이터프레임은 대게 여러 개의 열을 가진다. 각 열은 시리즈 형태로 끄집어내 사용할 수 있다.  
인덱스 연산자('[]')를 사용하면 어떤 열 이름도 사용할 수 있다. 

## 시리즈 메서드 호출
시리즈가 가진 모든 속성과 메서드를 찾아보려면 내장된 dir함수를 사용하면 된다.  
다음 코드는 시리즈와 데이터프레임에서 공통적인 속성과 메서드 개수를 보여준다.  
이 두 객체는 상당수의 속성과 메서드 이름을 공유한다. 

``` python
s_attr_methods = set(dir(pd.Series))
len(s_attr_methods)
df_attr_methods = set(dir(pd.DataFrame))
len(df_attr_methods)
len(s_attr_methods & df_attr_methods)
```
```python
movies = pd.read_csv('../data/movie.csv')
director = movies['director_name']
fb_likes = movies['actor_1_facebook_likes']
director.sample(n=5, random_state=42)
director.shape
director.unique()
director.count()
fb_likes.quantile()
fb_likes.min()
fb_likes.max()
fb_likes.mean()
fb_likes.median()
fb_likes.std()
fb_likes.describe()
fb_likes.quantile(.2)
fb_likes.quantile([.1, .2, .3, .4, .5, .6, .7, .8, .9])
director.isna()
fb_likes_filled = fb_likes.fillna(0)
fb_likes_filled.count()
fb_likes_dropped = fb_likes.dropna()
fb_likes_dropped.size
```

일반적으로 시리즈의 데이터 형식에 따라 가장 유용한 메서드가 달라진다. 예를 들어 object 데이터 형식의 시리즈에 가장 유용한 메서드 중 하나는 빈도를 계산하는 .value_counts()다.  
시리즈의 원소 개수는 .size나 .shape 속성이나 내장된 len 함수를 사용해 계산할 수 있다.  
.unique() 메서드는 고유한 원소를 담은 넘파이 배열을 반환한다.   
.count() 메서드는 결측치가 아닌 아이템 개수를 반환한다.  
.min(), .max(), .mean(), .median(), .std() 등의 기본적 통계량도 제공된다.  
.describe() 메서드를 사용해 요약 통계량과 함께 몇 가지 분위수를 동시에 표현할 수 있다. 다만 object 데이터 형식의 열에 적용하면 완전히 다른 출력이 반환된다.   
.quantile() 메서드는 수치 데이터의 분위수를 계산한다. 입력이 스칼라면 출력도 스칼라지만 입력이 리스트면 출력은 시리즈다.  
.isna() 메서드를 사용하면 각 개별 값의 결측치 여부를 알 수 있다. 결과는 불리언 배열 시리즈다.  
.fillna() 메서드를 사용하면 시리즈 내의 모든 결측치를 다른 값으로 대체할 수 있다.  
.dropna() 메서드를 사용하면 시리즈 내의 결측치를 제거한다.   
각 메서드에서 반환되는 객체들은 형식이 서로 다르다.  
.value_counts() 메서드는 가장 유익한 시리즈 메서드 중 하나다. 기본 설정으로는 개수를 반환하지만 normalize 매개변수를 True로 설정하면 개수 대신 상대빈도값이 반환된다.  

```python
director.value_counts(normalize=True)
director.hasnans
director.notna()
```
.hasnans 속성으로 결측치 여부를 검사할 수 있다.  
.notna() 메서드는 결측치가 아닌 모든 값에 대해 True를 반환한다. 

## 시리즈 연산
``` python
movies = pd.read_csv('../data/movie.csv')
imdb_score = movies['imdb_score']
imdb_score + 1
imdb_score * 2.5
imdb_score // 7
imdb_score > 7
director = movies['director_name']
director == 'James Cameron'
imdb_score.add(1)   # imdb_score + 1
imdb_score.gt(7)   # imdb_score > 7
```

뺄셈, 곱셈, 나눗셈, 지수와 같은 기본 산술 연산자도 스칼라 값과 유사하게 작동한다.  
파이썬에서 //는 몫을 구하는 나눗셈 연산이다. 이 연산은 결과를 내림한다.  
% 기호는 나머지를 구하는 연산으로, 나눗셈을 하고 난 나머지를 반환한다. 시리즈 인스턴스는 이 연산들을 지원한다.  
6개의 비교연산자도 존재하는데 각 비교연산자는 각 조건 결과에 기반을 두고 시리즈 각 값에 대해 True나 False를 반환한다.  
결과는 불리언 배열이며, 필터링에서 매우 유용하게 쓰인다.  
모든 연산자에는 동일한 결과를 생성하는 메서드가 있다. 연산자 대신 메서드를 사용하면 메서드를 체인시킬 때 유용하다.  
연산자로 뺄셈을 하면 결측치가 무시된다. 그러나 .sub() 메서드를 사용하면 결측치 대신 사용할 fill_value 매게변수를 지정할 수 있다.  
- +, -, *, /, //, %, ** : .add(), .sub(), .mul(), .div(), .floordiv(), .mod(), .pow()
- <, >, <=, >=, ==, != : .lt(), .gt(), .le(), .ge(), .eq(), .ne()
특정 시리즈의 각 요소에 2.5를 곱해야 한다는 것을 파이썬은 어떻게 알 수 있을까?  
파이썬의 객체는 특수 메서드를 사용해 연산자와 통신할 수 있는 표준화된 내장방법을 갖고 있다.  
특수 메서드는 객체가 연산자를 만날 때마다 내부적으로 호출된다. 특수 메서드는 항상 밑줄 두 개로 시작하고 끝난다. 따라서 던더 메서드라고도 한다.  
파이썬은 imdb_score * 2.5 표현식을 imdb_score.__mul__(2.5)로 해석한다.   
참고로 .mul() 메서드를 호출하는 것은 .__mul__() 메서드를 호출하는 것과 다르다. 

## 시리즈 메서드 체인
파이썬에서 모든 변수는 객체를 참조하며, 여러 속성과 메서드는 새로운 객체를 반환한다. 이 덕분에 속성 접근을 사용해 메서드를 연속으로 호출할 수 있다.  
이러한 방법을 메서드 체인 또는 플로우 프로그래밍이라 부른다. 
```python
movies = pd.read_csv('../data/movie.csv')
fb_likes = movies['actor_1_facebook_likes']
director = movies['director_name']
director.value_counts().head(3)
fb_likes.isna().sum()
fb_likes.dtype
(fb_likes.fillna(0)
         .astype(int)
         .head()
)
```
체인 끝에 추가되는 가장 흔한 두 가지 메서드는 .head()나 .sample()이다.  
결측치의 개수를 알아내는 일반적인 방법은 .isna()를 호출한 후 .sum()메서드를 체인시키는 것이다.  
.astype() 메서드를 사용해 데이터 타입을 바꿀 수 있다. 
체인의 잠재적 단점 중 하나는 디버깅이 어렵다는 것이다.   
메서드 호출 중에 생성된 중간 객체는 별도의 변수에 저장되지 않으므로 체인 중에서 발생한 오류의 정확한 위치를 추적하기 어렵다.  
체인 디버깅을 위한 옵션은 .pipe() 메서드를 호출해 중간값을 표시하는 것이다.  
시리즈에 대한 .pipe() 메서드에서는 시리즈를 입력으로 취하는 함수를 전달해야만 하고 함수의 출력은 무엇이든 가능하다. 

```python
(fb_likes.fillna(0)
         #.astype(int)
         #.head()
)
(fb_likes.fillna(0)
         .astype(int)
         #.head()
)
fb_likes.fillna(0) \
        .astype(int) \
        .head()
def debug_df(df):
    print("BEFORE")
    print(df)
    print("AFTER")
    return df
(fb_likes.fillna(0)
         .pipe(debug_df)
         .astype(int) 
         .head()
)
intermediate = None
def get_intermediate(df):
    global intermediate
    intermediate = df
    return df
res = (fb_likes.fillna(0)
         .pipe(get_intermediate)
         .astype(int) 
         .head()
)
```
중간값을 저장하는 전역 변수를 생성하고 .pipe() 메서드를 사용할 수 있다. 
체인을 괄호로 묶는 것을 선호한다. 체인에 메서드를 추가할 때마다 후행 백슬래시를 계속 추가해야 하는 것은 성가신 일이다. 

## 열 이름 변경
```python
movies = pd.read_csv('../data/movie.csv')
col_map = {'director_name':'Director Name', 
             'num_critic_for_reviews': 'Critical Reviews'} 
movies.rename(columns=col_map).head()
```
데이터 프레임의 .rename() 메서드를 사용하면 열 레이블의 이름을 바꿀 수 있다.  
columns 속성에 대입하면 열 이름이 바뀐다. 그러나 이 대입을 체인시킬 수 없다. 

```python
idx_map = {'Avatar':'Ratava', 'Spectre': 'Ertceps',
  "Pirates of the Caribbean: At World's End": 'POC'}
col_map = {'aspect_ratio': 'aspect',
  "movie_facebook_likes": 'fblikes'}
(movies
   .set_index('movie_title')
   .rename(index=idx_map, columns=col_map)
   .head(3)
)
movies = pd.read_csv('../data/movie.csv', index_col='movie_title')
ids = movies.index.tolist()
columns = movies.columns.tolist()

ids[0] = 'Ratava'
ids[1] = 'POC'
ids[2] = 'Ertceps'
columns[1] = 'director'
columns[-2] = 'aspect'
columns[-1] = 'fblikes'
movies.index = ids
movies.columns = columns

def to_clean(val):
    return val.strip().lower().replace(' ', '_')

movies.rename(columns=to_clean).head(3)
cols = [col.strip().lower().replace(' ', '_')
        for col in movies.columns]
movies.columns = cols
movies.head(3)
```
원하는 경우 .rename()를 사용해 인덱스의 이름을 바꿀 수도 있다.  
행과 열 레이블의 이름을 바꾸는 방법에는 여러 가지가 있다. 인덱스와 열 속성을 파이썬 리스트에 다시 대입할 수도 있다.  
이 경우 리스트에 행과 열 레이블과 동일한 개수의 원소가 있을 때 작동한다. 
.rename() 메서드에 함수를 전달할 수도 있다. 이 함수는 열 이름을 취한 뒤 새 이름을 반환한다.  
리스트 컴프리핸션도 사용할 수 있다. 새로 정리된 목록을 .columns 특성으로 다시 할당하면 된다. 

## 열의 생성과 삭제

.assign() 메서드를 사용해 새 열을 만든 다음 .drop() 메서드를 사용해 열을 삭제한다. 
새 열을 만드는 한 가지 방법은 인덱스 대입을 실행하는 것이다. 기본적으로 새 열은 마지막에 추가된다. 
```python
movies = pd.read_csv('../data/movie.csv')
movies['has_seen'] = 0
```
체인을 많이 사용하는 경우 .assign() 메서드를 사용하자. 이 메서드는 새 열이 추가된 새로운 데이터프레임을 반환한다.  
이 메서드는 매개변수 이름을 열 이름으로 사용하므로 열 이름은 유효한 매개변수 이름이어야 한다. 
```python
idx_map = {'Avatar':'Ratava', 'Spectre': 'Ertceps',
  "Pirates of the Caribbean: At World's End": 'POC'}
col_map = {'aspect_ratio': 'aspect',
  "movie_facebook_likes": 'fblikes'}
(movies
   .rename(index=idx_map, columns=col_map)
   .assign(has_seen=0)
)
```
페이스북의 '좋아요' 개수에 대한 데이터를 합산해 total_likes 열에 할당하자.
``` python
total = (movies['actor_1_facebook_likes'] +
         movies['actor_2_facebook_likes'] + 
         movies['actor_3_facebook_likes'] + 
         movies['director_facebook_likes'])
```
이번엔 체인할 수 있는 메서드를 사용해보자. 
```python
cols = ['actor_1_facebook_likes','actor_2_facebook_likes',
    'actor_3_facebook_likes','director_facebook_likes']
sum_col = movies[cols].sum(axis='columns')
sum_col.head(5)
movies.assign(total_likes=sum_col).head(5)
```
또 다른 옵션은 .assign() 메서드 호출에서 매개변수 값으로 함수를 전달하는 것이다. 이 함수는 데이터프레임을 입력으로 받고 시리즈를 반환한다.
```python
def sum_likes(df):
   return df[[c for c in df.columns
              if 'like' in c]].sum(axis=1)
movies.assign(total_likes=sum_likes).head(5)
```
이 데이터셋에는 결측치가 있다. + 연산자를 사용하면 결측치가 있을 경우 결과는 NaN이 된다.  
그러나 .sum() 메서드를 사용하면 NaN을 0으로 반환한다. 
```python
(movies
   .assign(total_likes=sum_col)
   ['total_likes']
   .isna()
   .sum()
)
(movies
   .assign(total_likes=total)
   ['total_likes']
   .isna()
   .sum()
)
(movies
   .assign(total_likes=total.fillna(0))
   ['total_likes']
   .isna()
   .sum()
)              
```
데이터셋에는 cast_total_facebook_likes라는 열이 있다.  
이 열 중 얼마나 많은 부분이 방금 새로 만든 열인 total_likes에서 왔는지 확인해보자.  
우선 데이터 유효성 검사를 수행하자.
```python
def cast_like_gt_actor_director(df):
    return df['cast_total_facebook_likes'] >= \
           df['total_likes']
df2 = (movies
   .assign(total_likes=total,
           is_cast_likes_more = cast_like_gt_actor_director)
)
df2['is_cast_likes_more'].all()
```
.all() 메서드를 사용하면 해당 열의 모든 값이 True인지 확인할 수 있다. 
이번엔 total_likes 컬럼을 삭제해보자.
```python
df2 = df2.drop(columns='total_likes')
```
배우들이 받은 '좋아요'만을 가진 새로운 시리즈를 생성하고 cast_total_facebook_likes에 있는 모든 값이 actor_sum보다 크거나 같은지 확인해보자.   
그런 다음 movie_title 열을 인덱스로 사용해 시리즈를 생성할 수 있다.  
시리즈 생성자에는 값과 인덱스 모두를 전달할 수 있다. 
```python
actor_sum = (movies
   [[c for c in movies.columns if 'actor_' in c and '_likes' in c]]
   .sum(axis='columns')
)
movies['cast_total_facebook_likes'] >= actor_sum
movies['cast_total_facebook_likes'].ge(actor_sum)
movies['cast_total_facebook_likes'].ge(actor_sum).all()
pct_like = (actor_sum
    .div(movies['cast_total_facebook_likes'])
)
pct_like.describe()
pd.Series(
    pct_like.to_numpy(), # or pct_like.values
    index=movies['movie_title'].values).head()
```

요약해보자.  
새 열을 생성할 때 스칼라를 대입하거나 시리즈를 대입할 수 있다. 
.drop() 메서드는 삭제할 행이나 열 이름을 취한다. 기본 설정은 인덱스 이름을 기준으로 행을 삭제한다.  
열을 삭제하려면 axis 매개변수를 1이나 'columns'로 설정해야 한다.  
축의 기본 설정값은 0이나 'index'이다.  
.insert() 메서드를 사용해 데이터프레임의 특정 위치에 새 열을 삽입할 수 있다.  
이 메서드는 새 열의 정수 위치를 첫 번째 인수로, 새 열의 이름을 두 번째 인수로, 값을 세 번째 인수로 취한다.  
열 이름의 정수 위치를 찾으려면 .get_loc 인덱스 메서드를 사용해야 한다.  
그리고 이 메서드는 데이터프레임 자체를 수정하므로 대입 명령문이 없다. 그리고 None을 반환한다.  
이러한 이유로 .assign() 메서드를 선호한다. 
각 영화의 수익을 계산하려면 총매출에서 비용을 제외한 후 gross 열 다음에 삽입하면 된다. 
```python
profit_index = movies.columns.get_loc('gross') + 1
profit_index
movies.insert(loc=profit_index,
              column='profit',
              value=movies['gross'] - movies['budget'])

del movies['director_name']
```
컬럼을 삭제하는 대안은 del문을 사용하는 것이다.  
그러나 이 또한 새로운 데이터프레임을 반환하지 않으므로 .drop() 메서드를 선호한다. 

# 2장 기본 데이터프레임 연산
## 여러 데이터프레임 열 선택
원하는 열의 리스트를 인덱스 연산자에 전달한다. 
```python
movies = pd.read_csv('../data/movie.csv')
movie_actor_director = movies[['actor_1_name', 'actor_2_name', 'actor_3_name', 'director_name']]
movie_actor_director.head()
type(movies.loc[:, ['director_name']]) # DataFrame
type(movies.loc[:, 'director_name']) # Series
```
.loc을 사용해 열 이름으로 꺼낼 수 있다. 콜론을 사용해 전체 행을 선택한다는 것을 명시해야 한다.  
인덱스 연산을 사용하면 시리즈나 데이터프레임을 반환할 수 있다.   
단일 아이템을 가진 리스트를 전달하면 데이터프레임을 반환받는다.  
문자열로 된 열의 이름을 전달하면 시리즈를 반환받는다.  
```python
cols = ['actor_1_name', 'actor_2_name',
        'actor_3_name', 'director_name']
movie_actor_director = movies[cols]
```
인덱스 연산자 내에 긴 리스트를 전달하면 가독성 문제가 발생할 수 있다.  
이를 피하고자 필요한 열 이름을 리스트 변수에 먼저 저장할 수 있다.  
판다스로 작업할 때 발생하는 가장 일반적인 예외 상황 중 하나는 KeyError다.  
이 오류는 주로 열이나 인덱스 이름이 잘못되었을 경우 발생한다.  

## 메서드를 사용해 열 선택
.select_dtypes()와 .filter() 메서드를 사용할 수 있다. 
```python
def shorten(col):
    return (col.replace('facebook_likes', 'fb')
               .replace('_for_reviews', '')
    )
movies = movies.rename(columns=shorten)
movies.dtypes.value_counts()
movies.select_dtypes(include='int').head()
movies.select_dtypes(include='number').head()
movies.select_dtypes(include=['int', 'object']).head()
movies.select_dtypes(exclude='float').head()
movies.filter(like='fb').head()
cols = ['actor_1_name', 'actor_2_name',
        'actor_3_name', 'director_name']
movies.filter(items=cols).head()
movies.filter(regex=r'\d').head()
```
.select_dtypes() 메서드를 사용해 특정 데이터 타입을 지닌 열을 선택할 수 있다.   
모든 수치 열만 선택하려면 include 매개변수에 number라는 문자열을 전달하면 된다.   
리스트로 넘겨서 정수와 문자열로 된 열을 얻을 수 있다.  
제외하고 싶을 땐 exclude 매개변수를 이용하자.  
열을 선택하는 다른 방법은 .filter() 메서들르 이용하는 것이다.  
like 매개변수는 열 이름에서 부분 문자열을 찾는다.  
items 매개변수를 사용하면 열 이름을 리스트로 전달할 수 있다. 이 매개변수는 거의 인덱스 연산을 복제한 것과 같다. 그러나 KeyError가 발생하지 않는다.  
그리고 regex 매개변수로 정규표현식을 사용해 열을 검색할 수 있다.  
items, like, regex 중 하나의 매개변수만 사용할 수 있다.  
.select_dtypes() 메서드의 혼란스러운 점 중 하나는 문자열과 파이썬 객체를 모두 사용할 수 있다는 유연성이다.  
둘 다 알아놓자.  
- np.number, 'number' : 크기와 상관없이 정수와 부동소수를 모두 선택
- np.float64, np.float_, float, 'float64', 'float_', 'float' : 64비트 부동소수점 수만 선택
- np.float16, np.float32, np.float128, 'float16', 'float32', 'float128' : 각각 정확히 16, 32, 128비트 부동소수점 수 선택
- np.floating, 'floating' : 크기와 상관없이 부동소수점 수 선택
- np.int0, np.int64, np.int_, int, 'int0', 'int64', 'int_', 'int' : 정확히 64비트 정수만 선택
- np.int8, np.int16, np.int32, 'int8', 'int16', 'int32' : 각각 정확히 8, 16, 32비트 정수 선택
- np.integer, 'integer' : 크기와 상관없이 모든 정수
- 'Int64' : null 값을 허용하는 정수로, 넘파이에는 없음
- np.object, 'object', 'O' : 모든 object 데이터 형식
- np.datetime64, 'datetime64', 'datetime' : 모든 datetime은 64비트
- np.timedelta64, 'timedelta64', 'timedelta' : 모든 timedelta는 64비트
- pd.Categorical, 'category' : 판다스에만 존재하고 넘파이에는 없음

## 열 이름 정렬
다음은 열을 정렬하는 지침이다.
- 각 열을 범주형이나 연속형으로 분류하라.
- 범주형과 연속형 열 내에서 공통된 열을 그룹화하라.
- 가장 중요한 열 그룹을 먼저 위치시키고 연속형보다 범주형 열을 먼저 위치시켜라. 

``` python
movies = pd.read_csv('../data/movie.csv')
def shorten(col):
    return (col.replace('facebook_likes', 'fb')
               .replace('_for_reviews', '')
    )
movies = movies.rename(columns=shorten)
cat_core = ['movie_title', 'title_year',
            'content_rating', 'genres']
cat_people = ['director_name', 'actor_1_name',
              'actor_2_name', 'actor_3_name']
cat_other = ['color', 'country', 'language',
             'plot_keywords', 'movie_imdb_link']
cont_fb = ['director_fb', 'actor_1_fb',
           'actor_2_fb', 'actor_3_fb',
           'cast_total_fb', 'movie_fb']
cont_finance = ['budget', 'gross']
cont_num_reviews = ['num_voted_users', 'num_user',
                    'num_critic']
cont_other = ['imdb_score', 'duration',
               'aspect_ratio', 'facenumber_in_poster']
new_col_order = cat_core + cat_people + \
                cat_other + cont_fb + \
                cont_finance + cont_num_reviews + \
                cont_other
set(movies.columns) == set(new_col_order)
movies[new_col_order].head()
```
새로운 열 순서를 담은 리스트가 원래 열을 모두 포함하도록 보장해야 한다.   
해들리 위컴은 먼저 고정 변수를 배치한 다음 측정 변수를 배치할 것을 제안했다.  

## 데이터프레임 요약
단일 열이나 시리즈 데이터를 대상으로 작동하는 다양한 메서가 있었다.  
그중 다수는 단일 스칼라 값을 반환하는 집계 혹은 축약 메서드였다.  
데이터프레임에서 이와 동일한 메서드를 호출하면 각 열에 대해 해당 작업을 한꺼번에 수행하고 데이터프레임의 각 열에 대한 결과를 축약한다.  
```python
movies = pd.read_csv('../data/movie.csv')
movies.shape
movies.size
movies.ndim
len(movies)
movies.count()
movies.min()
movies.describe().T
movies.describe(percentiles=[.01, .3, .99]).T
```
.shape 속성은 행과 열 수를 가진 튜플을 반환한다.  
.size 속성은 데이터프레임의 총 원소 개수를 반환하는데 행과 열 개수의 곱이다.  
.ndim 속성은 차원수를 반환하는데 모든 데이터프레임은 2이다.  
.count() 메서드는 각 열에서 결측치가 아닌 데이터 개수를 보여준다.  
이는 각 열을 하나의 단일값으로 요약하므로 집계 메서드다.  
출력은 열 이름을 인덱스로 갖는 시리즈다.  
요약 통계량을 계산하는 메서드들은 수치 열의 열 이름을 인덱스로 하고 각 집계치를 값으로 갖는 시리즈를 반환한다.  
.describe() 메서드는 기술통계량과 분위수를 동시에 계산한다.  
이 메서드는 수치 열의 요약 통계량을 표시한다.  
.T를 사용해 결과를 전치시키면 화면에 더 많은 정보를 표시할 수 있다.  
percentiles 매개변수를 사용하면 .describe() 메서드에 정확한 분위수를 지정할 수 있다.  
판다스 기본설정에서 숫자 열에서 누락된 값은 건너뛰고 처리한다.  
```python
movies.min(skipna=False)
```
skipna 매개변수를 False로 설정하면 적어도 하나의 결측치가 존재하는 경우 판다스는 모든 집계 메서드에서 NaN을 반환한다. 

## 데이터프레임 메서드 체인
메서드 체인의 가장 핵심은 체인의 각 단계에서 반환되는 정확한 객체를 아는 것이다.   
판다스에서는 거의 항상 데이터프레임이나 시리즈 또는 스칼라 값이다.  
```python
movies = pd.read_csv('../data/movie.csv')
def shorten(col):
    return (col.replace('facebook_likes', 'fb')
               .replace('_for_reviews', '')
    )
movies = movies.rename(columns=shorten)
movies.isnull().head()
(movies
   .isnull()
   .sum()
   .head()
)
movies.isnull().sum().sum()
movies.isnull().any().any()
```
전체 결측치의 개수를 스칼라 값으로 확인하고 싶으면 .isnull().sum().sum()  
데이터프레임에 결측치가 있는지 확인하려면 .isnull().any().any()  
object 데이터 형식이며 결측치를 가진 열은 집계 메서드(.min(), .max(), .sum())로부터 아무것도 반환하지 못한다.  
따라서 결측치를 채워야 한다.  
```python
with pd.option_context('max_colwidth', 20):
    movies.select_dtypes(['object']).fillna('').max()
```

## 데이터프레임 연산
판다스는 뱅커 반올림 연산을 한다. 즉 숫자가 양쪽의 한 가운데 있을 경우 가까운 짝수 쪽으로 만들어 버린다.  
소수 두 자리수로 반올림할 때 이 시리즈의 UGDS_BLACK 행이 어떻게 되는지 살펴보자. 
```python
colleges = pd.read_csv('../data/college.csv', index_col='INSTNM')
college_ugds = colleges.filter(like='UGDS_')
college_ugds.head()
name = 'Northwest-Shoals Community College'
college_ugds.loc[name]
college_ugds.loc[name].round(2)
(college_ugds.loc[name] + .0001).round(2)
college_ugds + .00501
(college_ugds + .00501) // .01
college_ugds_op_round = (college_ugds + .00501) // .01 / 100
college_ugds_op_round.head()
college_ugds_round = (college_ugds + .00001).round(2)
college_ugds_round
college_ugds_op_round.equals(college_ugds_round)
```
판다스가 뱅커 반올림하기 전에 .0001을 더하면 반올림된다. 
소수 2번째 자리까지 나타내려고 할 때 수학적으로 .005를 더하면 다음 단계의 정수 나눗셈이 가장 가까운 정수 백분율로 반올림되게 하는 데 충분하다.  
그러나 부동소수점의 부정확성으로 인해 문제가 발생한다.  
부동소수점 표현의 첫 번째 4자리 숫자가 실제 값과 동일하게 각 숫자에 .00001이 더하면 된다.  
해당 데이터셋에서 모든 점의 최대 정밀도가 소수점 이하 네 자리이기 때문이다. 
뱅커 반올림은 숫자가 지속적으로 더 높은 쪽으로 치우치는 것을 막아준다.   
.equals() 메서드는 두 데이터프레임 사이의 모든 요소와 인덱스가 정확히 동일한지 여부를 판별하고 불리언을 반환한다.   
결론은 소수점을 반올림 하고 싶으면 최대 정밀도보다 하나 더 큰 정밀도로 소수를 더해줘서 round 메서드를 사용하면 된다. 
```python
college2 = (college_ugds
    .add(.00501) 
    .floordiv(.01) 
    .div(100)
)
college2.equals(college_ugds_op_round)
```
시리즈와 마찬가지로 데이터프레임에도 연산자와 동등한 메서드가 있다.

## 결측치 비교
판다스는 np.nan 객체를 사용해 결측치를 나타낸다. 이 객체는 자신과 같지 않다. 
```python
np.nan == np.nan # False
```
시리즈와 데이터프레임은 요소별 비교를 위해 '같음' 연산자 ==를 사용한다.  
결과는 동일한 차원의 객체다.  

```python
college = pd.read_csv('../data/college.csv', index_col='INSTNM')
college_ugds = college.filter(like='UGDS_')
college_ugds == .0019
college_self_compare = college_ugds == college_ugds
college_self_compare.head()
college_self_compare.all()
(college_ugds == np.nan).sum()
college_ugds.isnull().sum()
college_ugds.equals(college_ugds)
```
등호 연산자는 결측치가 있는 데이터프레임과 비교할 때 문제가 된다.  
자기 자신을 등호를 사용해 연산하면 .all() 메서드를 사용한 결과에 모두 True가 있을 것 같지만 결과는 전혀 다르다.  
이 문제는 결측치가 서로 같은 것으로 여겨지지 않기 때문이다.  
등호 연산자를 사용해 결측치를 '같음' 연산자를 사용해 세고 불리언 열을 합산하려고 하면 각각에 대해 0을 얻게 된다.  
이 또한 당연한 결과다.  
따라서 결측치 수를 얻는 데 등호 연산자가 아닌 .isna() 메서드를 사용하라.   
두 개의 전체 데이터프레임을 서로 비교하는 올바른 방법은 등호 연산자가 아니라 .equals() 메서드를 사용하는 것이다.  
이 메서드는 같은 위치에 있는 NaN을 동일하게 취급한다.  

``` python
college_ugds.eq(.0019)    # same as college_ugds == .0019
from pandas.testing import assert_frame_equal
assert_frame_equal(college_ugds, college_ugds) is None
```
.eq() 메서드는 == 연산자와 마찬가지로 요소별 비교를 수행한다.  
.eq() 메서드는 .equals() 메서드와 전혀 다르다.  
pandas.testing 서브패키지 안에는 개발자들이 단위 테스트를 생성할 때 사용해야 하는 함수가 있다.  
asert_frame_equal 함수는 두 개의 데이터프레임이 같이 않으면 AssertionError를 발생시킨다.  
만약 같은 경우 None을 반환한다. 

## 데이터프레임 연산 방향 전환
많은 데이터프레임 메서드에는 axis 매개변수가 있다. 이 매개변수는 연산이 진행되는 방향을 제어한다.  
axis 매개변수는 'index'(또는 0) 또는 'columns'(또는 1)이다.  
거의 모든 데이터프레임 메서드는 기본적으로 axis 매개변수를 0으로 설정하며 인덱스를 따르는 작업에 적용된다.  
```python
college = pd.read_csv('../data/college.csv', index_col='INSTNM')
college_ugds = college.filter(like='UGDS_')
college_ugds.head()
college_ugds.count()
college_ugds.count(axis='columns').head()
college_ugds.sum(axis='columns').head()
college_ugds.median(axis='index')

college_ugds_cumsum = college_ugds.cumsum(axis=1)
college_ugds_cumsum.head()
```
UGDS로 시작하는 열은 특정 인종의 학부생 비율을 나타낸다. .filter() 메서드를 사용해 이 열들을 선택한다.  
.count() 메서드는 결측치가 아닌 개수를 반환한다.  
axis 매개변수를 'columns'로 변경하면 연산의 방향이 변경된다.  
결측치를 계산하는 대신 각 행의 모든 값을 합할 수 있다. 이 때 각 백분율 행은 합산이 1이 되어야 한다. .sum() 메서드를 사용해 확인할 수 있다.  
각 열의 분포를 알아보려면 .median() 메서드를 사용할 수 있다.  
.cumsum() 메서드는 누적합을 보여준다.  

## 대학 인종 다양성 지수 결정
```python
college = pd.read_csv('../data/college.csv', index_col='INSTNM')
college_ugds = college.filter(like='UGDS_')
(college_ugds.isnull()
   .sum(axis='columns')
   .sort_values(ascending=False)
   .head()
)
college_ugds = college_ugds.dropna(how='all')
college_ugds.isnull().sum()
college_ugds.ge(.15)
diversity_metric = college_ugds.ge(.15).sum(axis='columns')
diversity_metric.head()
diversity_metric.value_counts()
diversity_metric.sort_values(ascending=False).head()
college_ugds.loc[['Regency Beauty Institute-Austin',
                   'Central Texas Beauty College-Temple']]
us_news_top = ['Rutgers University-Newark',
                  'Andrews University',
                  'Stanford University',
                  'University of Houston',
                  'University of Nevada-Las Vegas']
diversity_metric.loc[us_news_top]

(college_ugds
   .max(axis=1)
   .sort_values(ascending=False)
   .head(10)
)
(college_ugds > .01).all(axis=1).any()
```
많은 대학에서 인종 열에 결측치가 있다. 모든 열에 대해 결측치 개수를 파악하고 결과 시리즈를 내림차순으로 정렬한다.  
.dropna() 메서드를 사용해 9개 인종 모두에 대한 값이 누락된 전체 행을 제거한다.  
'크거나 같다'의 데이터프레임 메서드인 .ge()를 사용해 각 셀에 대한 불리언 값을 가진 데이터프레임을 구한다.  
.sum() 메서드를 사용해 각 대학의 True 값을 알아본다. 그리고 .value_counts() 메서드를 통해 분포를 파악한다.  
두 학교가 5가지 인종 범주에 대해 15% 이상의 학생을 갖고 있다.  
해당 학교를 .loc 인덱스로 선택해보자.  
범주를 알 수 없는 둘 이상의 인종 열이 집계되었다.    
.dropna() 메서드에는 how라는 매개변수가 있으며 기본 설정 값은 문자열 'any'이지만 'all'로 변경할 수 있다.  
'any'로 설정하면 하나 이상의 결측치가 포함된 행이 삭제된다.  
'all'로 설정하면 모든 값이 누락된 행만 삭제된다.  
가장 다양화되지 않은 학교도 착을 수 있다.  
그리고 9개 범주 모두가 1%를 넘는 학교도 알아볼 수 있다.  

# 3장 데이터프레임 생성과 유지
## 스크래치에서 데이터프레임 생성
'''python
fname = ['Paul', 'John', 'Richard', 'George']
lname = ['McCartney', 'Lennon', 'Starkey', 'Harrison']
birth = [1942, 1940, 1940, 1943]
people = {'first': fname, 'last': lname, 'birth': birth}
beatles = pd.DataFrame(people)
beatles
```
데이터를 담은 병렬 리스트를 생성한다. 리스트 각각은 데이터프레임의 열이 되므로 동일한 데이터 타입을 가져야 한다.  
다음에 딕셔너리를 생성하고 열 이름을 리스트와 매핑한다.  
딕셔너리를 데이터프레임으로 만든다.  

```python
beatles.index
pd.DataFrame(people, index=['a', 'b', 'c', 'd'])

pd.DataFrame(
[{"first":"Paul","last":"McCartney", "birth":1942},
 {"first":"John","last":"Lennon", "birth":1940},
 {"first":"Richard","last":"Starkey", "birth":1940},
 {"first":"George","last":"Harrison", "birth":1943}])

 pd.DataFrame(
     [{"first":"Paul","last":"McCartney", "birth":1942},
 {"first":"John","last":"Lennon", "birth":1940},
 {"first":"Richard","last":"Starkey", "birth":1940},
 {"first":"George","last":"Harrison", "birth":1943}],
 columns=['last', 'first', 'birth'])
```
기본 설정으로 판다스는 데이터프레임의 생성자가 호출되면 RangeIndex를 생성한다.   
원한다면 데이터프레임에 다른 인덱스를 지정할 수도 있다.  
그리고 딕셔너리 여러 개를 담은 리스트로도 데이터프레임을 만들 수 있다.  
필요하면 columns 매개변수를 사용해 열 순서를 지정할 수 있다.  

## CSV 작성
CSV의 장점은 다음과 같다. 
- 사람이 읽을 수 있다.
- 텍스트 편집기에서 열린다.
- 대부분의 소프트웨어 스프레드시트에서 읽을 수 있다.
CSV의 단점은 다음과 같다.
- CSV 표준이 없어 인코딩이 이상해보일 수 있다.
- 형식을 지정할 수 없다.
- 텍스트 기반이라 용량이 매우 커질 수 있다(물론 압축 가능하다).
데이터프레임에는 to_로 시작하는 몇 개의 메서드가 있는데 이들은 데이터프레임을 export 한다. 
```python
from io import StringIO
fout = StringIO()
beatles.to_csv(fout)  # use a filename instead of fout
print(fout.getvalue())
```

.to_csv() 메서드에는 몇 가지 옵션이 있다.  
index_col 매개변수가 있어 인덱스의 위치를 지정할 수 있다.  
또한 인덱스를 포함하고 싶지 않다면 index 매개변수를 False로 설정할 수 있다. 
```python
_ = fout.seek(0)
pd.read_csv(fout)
_ = fout.seek(0)
pd.read_csv(fout, index_col=0)
fout = StringIO()
beatles.to_csv(fout, index=False) 
print(fout.getvalue())
```

## 대형 CSV 파일 읽기
판다스 라이브러리는 인메모리 도구다. 판다스로 데이터를 다루려면 먼저 모두 메모리로 읽어야 한다.  
대규모 CSV 파일이라면 몇 가지 옵션이 있다.  
한 번에 일부만 처리할 수 있다면 부분만 읽은 다음 각 부분을 처리하면 된다.  
다른 방법은 파일 크기를 축소시키는 힌트를 사용하는 것이다.  
```python
diamonds = pd.read_csv('../data/diamonds.csv', nrows=1000)
diamonds.info()
diamonds2 = pd.read_csv('../data/diamonds.csv', nrows=1000,
    dtype={'carat': np.float32, 'depth': np.float32,
           'table': np.float32, 'x': np.float32,
           'y': np.float32, 'z': np.float32,
           'price': np.int16})
diamonds2.info()
diamonds.describe()
diamonds2.describe()
diamonds2.cut.value_counts()
diamonds2.color.value_counts()
diamonds2.clarity.value_counts()
diamonds3 = pd.read_csv('../data/diamonds.csv', nrows=1000,
    dtype={'carat': np.float32, 'depth': np.float32,
           'table': np.float32, 'x': np.float32,
           'y': np.float32, 'z': np.float32,
           'price': np.int16,
           'cut': 'category', 'color': 'category',
           'clarity': 'category'})
diamonds3.info()
np.iinfo(np.int8)
np.finfo(np.float16)
cols = ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'price']
diamonds4 = pd.read_csv('../data/diamonds.csv', nrows=1000,
    dtype={'carat': np.float32, 'depth': np.float32,
           'table': np.float32, 'price': np.int16,
           'cut': 'category', 'color': 'category',
           'clarity': 'category'},
    usecols=cols)
diamonds4.info()
cols = ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'price']
diamonds_iter = pd.read_csv('../data/diamonds.csv', nrows=1000,
    dtype={'carat': np.float32, 'depth': np.float32,
           'table': np.float32, 'price': np.int16,
           'cut': 'category', 'color': 'category',
           'clarity': 'category'},
    usecols=cols,
    chunksize=200)
def process(df):
    return f'processed {df.size} items'
for chunk in diamonds_iter:
    print(process(chunk))
```
nrows 매개변수를 사용하면 읽을 데이터를 작은 표본으로 제한할 수 있다.  
.info() 메서드를 사용해 데이터 표본이 사용하고 있는 메모리 용량을 알 수 있다.  
read_csv에 dtype 매개변수를 사용해 정확한(또는 더 적은) 수치 형식을 사용하도록 지정하자.  
수치형식을 변경하지 메모리 절약이 가능해졌다.  
dtype 매개변수를 사용해 객체 형식을 범주형으로 변경해보자. 먼저 각 object열의 .value_counts() 메서드를 살펴보자.  
개수가 몇 개 없다면 범주형으로 변경해 메모리를 더 절약할 수 있다.  
무시해도 되는 열은 usecols 매개변수를 사용해 제외해보자.  
위 단계들을 거쳤는데도 여전히 메모리가 부족한가? 만약 한 번에 전체 데이터를 메모리에 읽어들이지 않고 일부만 처리해도 된다면 chuncksize 매개변수를 쓰자.  
CSV 파일에는 데이터 형식에 대한 정보가 없으므로 판다스가 유추한다.  
열의 모든 값이 정수고 결측치가 없다면 int64를 사용한다.  
열이 수치지만 정수가 아닌 경우 또는 결측치가 있다면 float64를 사용한다.  
열이 수치가 아니면 판다스는 열을 object 열로 변환하고 값을 문자열로 취급한다.  
판다스의 문자열 값은 각각 파이썬 문자열로 저장되므로 많은 메모리를 차지한다.  
이를 범주형으로 변환하면 메모리를 절약할 수 있다.  
또한 read_csv() 함수에 URL을 직접 전달하여 인터넷상의 CSV 파일을 읽을 수 있다.  
iinfo() 함수를 사용하면 넘파이 integer 형식의 범위를 볼 수 있으며 finfo() 함수로 부동소수점 형식의 정보를 볼 수 있다.  

```python
diamonds.price.memory_usage()
diamonds.price.memory_usage(index=False)
diamonds.cut.memory_usage()
diamonds.cut.memory_usage(deep=True)
diamonds4.to_feather('/tmp/d.arr')
diamonds5 = pd.read_feather('/tmp/d.arr')
diamonds4.to_parquet('/tmp/d.pqt')
```

.memory_usage() 메서드를 사용하면 메모리 사용량을 알 수 있으며 deep=True를 전달하면 객체 형식 시리즈의 사용량을 알 수 있다.  
원하는 형식으로 데이터를 가져오면 Feather 형식과 같이 형식을 추적하는 이진형식으로 저장할 수 있다.  
판다스는 pyarrow 라이브러리를 사용해 수행한다.  
이 형식은 언어 간에 구조화된 데이터를 메모리 내에서 전송할 수 있게 하기 위한 것이다.  
내부 변환 없이 데이터를 그대로 사용할 수 있도록 최적화됐다.  
이 형식으로 일단 정의하고 나면 훨씬 빠르게 쉽게 읽어들일 수 있다.  
또 다른 이진 옵션으로는 Parquet 형식이 있다.  
Feather가 인메모리 구조의 이진 데이터를 최적화하는 반면 Parquet은 온디스크 형식을 최적화한다.  
둘 다 CSV보다 빠르며 형식을 유지한다.  

## 엑셀 파일 사용
엑셀 파일로 읽고 저장하려면 xlwt 또는 openpyxl을 설치해야 한다.  
```python
beatles.to_excel('/tmp/beat.xls') # 더이상 지원 안함
beatles.to_excel('/tmp/beat.xlsx')
beat2 = pd.read_excel('/tmp/beat.xls')
beat2
beat2 = pd.read_excel('/tmp/beat.xls', index_col=0)
beat2
beat2.dtypes
```
xlwt는 앞으로 더 이상 사용되지 않을 예정이다. 따라서 openpyxl 라이브러리를 다운 받고 '~.xlsx' 로 저장해야 한다.  
xlrd도 마찬가지로 사용하지 말자. openpyxl은 .xlsx 파일을 읽고 쓸 수 있는 라이브러리다.  

```python
xl_writer = pd.ExcelWriter('/tmp/beat.xlsx')
beatles.to_excel(xl_writer, sheet_name='All')
beatles[beatles.birth < 1941].to_excel(xl_writer, sheet_name='1940')
xl_writer.save()
```
.to_excel() 메서드에 sheet_name 매개변수를 전달하면서 시트의 이름을 지정할 수 있다. 

## ZIP 파일로 작업
```python
autos = pd.read_csv("data/vehicles.csv.zip")
autos['modifiedOn'] = pd.to_datetime(auto['modifiedOn'])
autos = pd.read_csv('data/vehicles.csv.zip', parse_dates=['modifiedOn'])
```
zip 파일에 csv파일 하나만 있다면 바로 읽을 수 있다.   
오브젝트 타입을 날짜 타입으로 변환하는 데 2가지 방법이 있다. 하나는 pd.to_datetime() 메서드를 사용하는 것이다.  
다른 하나는 parse_dates 인자에 컬럼 값을 리스트에 넣어서 csv파일을 읽는 것이다.  

```python
import zipfile

with zipfile.ZipFile(
    'data/kaggle-survey-2018.zip'
) as z:
    print('\n'.join(z.namelist()))
    kag = pd.read_csv(
        z.open('multipleChoiceResponses.csv')
    )
    kag_questions = kag.iloc[0]
    survey = kag.iloc[1:]
```
read_csv 함수는 ZIP 파일 내부의 파일을 지정할 수 있는 기능이 없다.  
대신 파이썬 표준 라이브러리에 있는 zipfile 모듈을 사용해야 한다.   
zipfile 모듈은 URL과는 작동하지 않는다. 따라서 ZIP 파일이 URL에 있으면 먼저 다운로드해야 한다.  

## 데이터베이스와 작업
데이터를 저장하기 위한 SQLite 데이터베이스를 생성한다.
```python 
import sqlite3
con = sqlite3.connect("d:/data/beat.db")

with con:
    cur = con.cursor()
    cur.execute("""DROP TABLE band""")
    cur.execute("""CREATE TABLE Band(
        id INTEGER PRIMARY KEY,
        fname TEXT,
        lname TEXT,
        birthyear INT
    )""")
    cur.execute("""INSERT INTO Band VALUES(0, 'Paul', 'McCartney', 1942)""")
    cur.execute("""INSERT INTO Band VALUES(1, 'John', 'Lennon', 1940)""")
    _ = con.commit()
```
이제 데이터베이스에서 테이블을 DataFrame으로 읽어본다.  
테이블을 읽으려면 SQLAlchemy와 연결이 필요하다. 이 라이브러리는 데이터베이스를 추상화한다. 
```python
import sqlalchemy as sa

engine = sa.create_engine(
    "sqlite:///data/beat.db", echo=True
)
sa_connection = engine.connect()
beat = pd.read_sql(
    "Band", sa_connection, index_col='id'
)

sql = """SELECT fname, birthyear from Band"""
fnames = pd.read_sql(sql, con)
```
판다스 라이브러리는 SQLAlchemy 라이브러리를 활용하는데 대부분의 SQL 데이터베이스와 소통할 수 있다.  

## JSON 파일 읽기
파이썬 표준 라이브러리에는 JSON에서 인코딩과 디코딩이 되는 json 라이브러리가 들어있다.  
```python
import json
encoded = json.dumps(people)
json.loads(encoded)

beatles = pd.read_json(encoded)
```
read_json() 함수를 이용해 데이터를 읽는다. json 파일을 읽을 때 알아둬야 할 것은 판다스가 로드할 수 있는 특정 형식이어야 한다는 것이다.  
판다스는 몇 가지 데이터 방향을 지원한다. 
- columns : (기본설정) 열 이름과 열에 있는 값 리스트의 매핑이다. 
- records : 행의 리스트로, 각 행은 열과 값을 매핑하는 딕셔너리다.  
- split : columns를 열 이름에, index를 인덱스 값에, data를 각 데이터의 행 리스트에 매핑한다.  
- index : 데이터에서 각 행의 리스트다. 열이나 인덱스 값을 포함하지 않는다.  
- table : schema를 DataFrame 스키마에, data를 딕셔너리의 리스트에 연결한다. 

다음은 이러한 형식의 예이다. 
```python
records = beatles.to_json(orient='records')
pd.read_json(records, orient='records')

split = beatles.to_json(orient='split')
pd.read_json(split, orient='split')

index = beatles.to_json(orient='index')
pd.read_json(index, orient='index')

values = beatles.to_json(orient='values')
pd.read_json(values, orient='values').rename(columns=dict(enumerate(['first', 'last', 'birth'])))

table = beatles.to_json(orient='table')
pd.read_json(table, orient='table')
```
JSON을 생성해야 하는 경우(웹 서비스 작성 등)라면 columns나 records 방향을 권장한다. 
만약 JSON에 데이터를 추가해야 하는 경우라면 .to_dict() 메서드를 사용해 딕셔너리를 생성하라.  
새 데이터를 딕셔너리에 추가한 다음 해당 딕셔너리를 JSON으로 변환할 수 있다.  
```python
output = beat.to_dict()
output['version'] = '0.4.1'
json.dumps(output)
```

## HTML 테이블 읽기

# 데이터 분석 시작
## 데이터 분석 루틴 개발












































































